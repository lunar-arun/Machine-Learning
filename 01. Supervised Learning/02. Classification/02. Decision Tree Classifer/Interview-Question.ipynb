{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "001cd77d",
   "metadata": {},
   "source": [
    "    What is a Decision Tree in machine learning, and how does it work?\n",
    "        It is a supervised learning technique used for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064ba751",
   "metadata": {},
   "source": [
    "    What is entropy?\n",
    "        It's a measure of impurity or disorder in a dataset. \n",
    "        \n",
    "        A low entropy indicates that the data is pure, meaning all instances in the subset belong to the same class, \n",
    "        while a high entropy suggests that the data is mixed with multiple classes.\n",
    "        \n",
    "        E(S) = − p1 ∗ log2(p1) − p2 ∗log2(p2) − . . . − pn ∗log2(pn)\n",
    "            E(S) is the entropy of the dataset S.\n",
    "            p1, p2, . . . pn are the proportions of different classes in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6b73d0",
   "metadata": {},
   "source": [
    "    what is Information Gain?\n",
    "        Measure of the reduction in entropy.\n",
    "        \n",
    "        A higher information gain implies that using attribute A for splitting leads to a more significant reduction in uncertainty (entropy), making it a good choice for the next split.\n",
    "        \n",
    "        I.G(S, A) = E(S) - Σ (|Sv|/|S|) * E(Sv)\n",
    "            I.G(S, A) is the information gain to achieve for splitting dataset S using A attribute.\n",
    "            E(S) is the entropy of original dataset S.\n",
    "            Sv is the number of instances in dataset S for which attribute A takes the value v.\n",
    "            S is the total number of instances in dataset S."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe76854a",
   "metadata": {},
   "source": [
    "        By selecting attributes that maximize information gain or minimize impurity, Decision Trees aim to create splits that separate the data into homogeneous subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cb4f35",
   "metadata": {},
   "source": [
    "    what is Gini Impurity?\n",
    "         measure of impurity or disorder in dataset.\n",
    "         Gini impurity helps determine the best attribute to split the data at each node of a Decision Tree.\n",
    "          Gini impurity takes values between 0 and 1.\n",
    "          Gini impurity of 0 indicates that the dataset is pure, meaning all elements belong to the same class.\n",
    "          Gini impurity of 1 means that the dataset is completely impure, with elements evenly distributed across all classes.\n",
    "          \n",
    "          Gini Impurtiy : \n",
    "              Gini(S) = 1 - Σ (p)^2\n",
    "          \n",
    "          The attribute with the lowest Gini impurity (or highest Gini gain) is preferred.\n",
    "          \n",
    "          Gini_gain(S, A) = Gini(S) - Σ (|Sv|/|S|) * Gini(Sv)\n",
    "              Gini_gain(S, A) is the Gini gain to achieve for splitting dataset S using A attribute.\n",
    "              Gini(S) is the entropy of original dataset S.\n",
    "              Sv is the number of instances in dataset S for which attribute A takes the value v.\n",
    "              S is the total number of instances in dataset S."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d326a5",
   "metadata": {},
   "source": [
    "    What are the advantages?\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a5421b",
   "metadata": {},
   "source": [
    "    What are the disadvantages?\n",
    "        Overfitting : \n",
    "            Occur due to capture of noise in the dataset, outliers letting to tree to grow deep or complex.\n",
    "            solution : \n",
    "                Pruning the tree: Pruning involves removing branches that do not significantly improve performance on validation data.\n",
    "                \n",
    "                Setting a maximum depth: Limiting the depth of the tree can prevent it from becoming overly complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdadf62",
   "metadata": {},
   "source": [
    "    what are the types of Decision Tree Algorithm?\n",
    "        ID3 (Iterative Dichotomiser 3) \n",
    "        \n",
    "        C4.5 (Classification and Regression Trees)\n",
    "        \n",
    "        CART (Classification and Regression Trees)\n",
    "        \n",
    "        CHAID (Chi-Squared Automatic Interaction Detection)\n",
    "        \n",
    "        MARS (Multivariate Adaptive Regression Splines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f79d5e6",
   "metadata": {},
   "source": [
    "    what is ID3 (Iterative Dichotomiser 3) method?\n",
    "        It uses entropy and information gain as criteria for attribute selection.\n",
    "        ID3 was primarily designed for categorical attributes and binary classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87454a85",
   "metadata": {},
   "source": [
    "    what is C4.5 (Classification and Regression Trees)?\n",
    "        CART can handle both categorical and numerical attributes and uses Gini impurity as the criterion for classification tasks and mean squared error for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547b9da5",
   "metadata": {},
   "source": [
    "    what is CHAID (Chi-Squared Automatic Interaction Detection)?\n",
    "        CHAID is primarily used for classification tasks.\n",
    "        CHAID is designed to work with categorical data and creates multiway trees rather than binary trees, making it useful for exploring interactions between multiple attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d378a067",
   "metadata": {},
   "source": [
    "    What is difference between Gini and Entropy?\n",
    "        Both measures the quality of split when building the decision tree\n",
    "        \n",
    "        Gini Impurity :\n",
    "            measure of probability of incorrectly classifying.\n",
    "            formula : 1 - Σ(P²)\n",
    "            range 0 to 0.5\n",
    "        Entropy : \n",
    "            measure of level of randomness.\n",
    "            formula : -P(+)*log(p(+)) -P(-)*log(P(-))\n",
    "            range 0 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e289f7",
   "metadata": {},
   "source": [
    "    what is min_sample_split?\n",
    "        \"min_samples_split\" is a hyperparameter that controls the minimum number of samples required to create a node split in a decision tree.\n",
    "        \n",
    "        Proper tuning of this parameter is essential for finding the right balance between model complexity and the ability to generalize to unseen data.\n",
    "        \n",
    "        Adjusting \"min_samples_split\" is part of the broader process of hyperparameter tuning to optimize the performance of your decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a174358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
