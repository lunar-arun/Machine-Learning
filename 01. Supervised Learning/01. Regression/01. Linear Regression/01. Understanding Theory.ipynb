{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ecded6e",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0259b95e",
   "metadata": {},
   "source": [
    "    A Supervised Machine Learning algorithm that predicts the continuous value by understanding the linear relationship between predictor and response variable.\n",
    "    \n",
    "    Linear Regression is a parametric model where we deal with assumption.\n",
    "    \n",
    "    Two types of LR :\n",
    "        Simple Linear Regression :\n",
    "        \n",
    "$$y = α + β x$$\n",
    "\n",
    "        Multiple Linear Regression :\n",
    "        \n",
    "$$y = α + β_1 x_1 + β_2 x_2 + β_3 x_3 +...+β_q x_q$$      \n",
    "\n",
    "    Parameters of Linear Regression :\n",
    "        α : intercept | β : Coefficent of Regression\n",
    "    \n",
    "    y --> variable(Dependent | Response | Outcome)\n",
    "    x --> variable(Independent | Predictor | Explanatory) \n",
    "    \n",
    "    These parameters are estimated by\n",
    "        Statistical Method Estimation : \n",
    "            OLS(Ordinal Least Square) : aim is to minimize the error of the predicted model.\n",
    "        \n",
    "        Optimal Method Estimation :\n",
    "            Gradient Descent\n",
    "        \n",
    "---        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503df1d6",
   "metadata": {},
   "source": [
    "    OLS(Ordinal Least Square)\n",
    "         Technique for estimating coefficients of linear regression equations.\n",
    "         \n",
    "         Least squares stand for the minimum squares error (SSE).\n",
    "         \n",
    "         OLS method aims to minimize the sum of square differences between the observed and predicted values. \n",
    "         \n",
    "    Understanding the Mathematics behind the OLS Algorithm\n",
    "    \n",
    "$$Y_i = \\alpha + \\beta X_i + \\varepsilon_i $$\n",
    "\n",
    "    Where εi is the error term, and α, β are the true (but unobserved) parameters of the regression.\n",
    "\n",
    "$$\\text{Error }(\\hat{\\varepsilon}_i) = \\hat{y_i} - y_i$$\n",
    "\n",
    "    The goal of simple linear regression is to find those parameters α and β for which the error term is minimized.\n",
    "    \n",
    "$$\\hat{\\alpha} = \\underset{\\hat{\\alpha}}{\\text{min}} \\sum_{i=1}^{n} \\left( y_i - (\\alpha + \\beta x_i) \\right)^2 = \\underset{\\hat{\\alpha}}{\\text{min}} \\sum_{i=1}^{n} \\left(\\varepsilon_i\\right)^2$$\n",
    "\n",
    "$$\\hat{\\beta} = \\underset{\\hat{\\beta}}{\\text{min}} \\sum_{i=1}^{n} \\left( y_i - (\\alpha + \\beta x_i) \\right)^2 = \\underset{\\hat{\\beta}}{\\text{min}} \\sum_{i=1}^{n} \\left(\\varepsilon_i\\right)^2$$\n",
    "\n",
    "    The equations for α and β that minimize the error term can be derived through calculus by taking partial derivatives with respect to α and β and setting them to zero. \n",
    "\n",
    "    This procedure is called ordinary least squares (OLS) error.\n",
    "    Advantages of OLS Regression\n",
    "        \n",
    "    Partial Derivative with respect to α:        \n",
    "$$ \\frac{\\partial}{\\partial \\alpha} = -2 \\sum_{i=1}^{n} (y_i - (\\alpha + \\beta x_i)) $$  \n",
    "\n",
    "    Setting partial derivative for α to zero:\n",
    "$$ \\sum_{i=1}^{n} y_i - n \\hat{\\alpha} - \\hat{\\beta} \\sum_{i=1}^{n} x_i = 0 $$    \n",
    "\n",
    "    Solving for α cap:\n",
    "$$\\hat{\\alpha} = \\frac{1}{n} \\left( \\sum_{i=1}^{n} y_i - \\hat{\\beta} \\sum_{i=1}^{n} x_i \\right)$$   \n",
    "\n",
    "    Partial Derivative with respect to β:\n",
    "$$\\frac{\\partial}{\\partial \\beta} = -2 \\sum_{i=1}^{n} x_i(y_i - (\\alpha + \\beta x_i)) $$   \n",
    "\n",
    "    Setting partial derivative for β to zero:\n",
    "$$\\sum_{i=1}^{n} x_i y_i - \\hat{\\alpha} \\sum_{i=1}^{n} x_i - \\hat{\\beta} \\sum_{i=1}^{n} x_i^2 = 0$$    \n",
    "\n",
    "    Solving for β cap:\n",
    "$$\\hat{\\beta} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$$    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971982d9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8024e1ed",
   "metadata": {},
   "source": [
    "    Assumption of Linear Regression\n",
    "    \n",
    "        Linearity\n",
    "        \n",
    "        Normality\n",
    "        \n",
    "        No Multicollinearity\n",
    "        \n",
    "        Homoscedasticity\n",
    "        \n",
    "        No Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde38be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d199be",
   "metadata": {},
   "source": [
    "    Linearity : \n",
    "        Understanding the linear association between each predictor with response variable.\n",
    "        \n",
    "        With Pearson's R Correlation, we can find the relation whether they are strongly or weakly correlated or not.\n",
    "        \n",
    "        Graphically we can visualize with scatter plot!        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8749e97",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f618e93",
   "metadata": {},
   "source": [
    "    Normality : \n",
    "        Reponse Variablie should be Normal Distributed  with Mean μ and Variance σ2.\n",
    "        \n",
    "        Residual | Error should be Normal Distributed  with Mean μ = 0 and Variance σ2.\n",
    "        \n",
    "        Graphically we can visualize with kdeplot or displot!\n",
    "        \n",
    "        Measure :\n",
    "            Anderson Darling Test\n",
    "            \n",
    "            Shapiro-Wilk Test\n",
    "            \n",
    "        When above measure fails, we have to transform the data to make it normal distributed.\n",
    "        \n",
    "        Transformation :\n",
    "            BOX-COX transformation\n",
    "            \n",
    "            Yeo-Johnson(data point are -ve) transformation\n",
    "            \n",
    "            Log transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2204fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ecbc14",
   "metadata": {},
   "source": [
    "    No Multicollinearity :\n",
    "        There should no correlation among independent variable. If we have multicollinearity then we get false significance.\n",
    "        \n",
    "        With Correlation, we can find the relation whether they are strongly or weakly correlated with each other or not.\n",
    "        \n",
    "        Measure :\n",
    "            Correlation \n",
    "            \n",
    "            Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe382d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34070c5b",
   "metadata": {},
   "source": [
    "    Homoscedasticiy : \n",
    "        Checks for Same Spreadness\n",
    "        \n",
    "        Meaning variance of error or residual should be statistically equal to zero\n",
    "        Measure : \n",
    "            Breusch Pagan Test\n",
    "                Ho: Homoscedasticity exists (constant variance of residuals).\n",
    "                Ha: Heteroscedasticity exists (non-constant variance of residuals)\n",
    "\n",
    "            \n",
    "            Goldfeld Quandt Test\n",
    "                Ho: Homoscedasticity (equal variance)\n",
    "                Ha: Heteroscedasticity (unequal variance)\n",
    "                \n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_goldfeldquandt\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Perform the Goldfeld-Quandt test\n",
    "gq_test = het_goldfeldquandt(model.resid, X)\n",
    "print(\"F-statistic:\", gq_test[0])\n",
    "print(\"p-value:\", gq_test[1])\n",
    "print(\"The null hypothesis of homoscedasticity is rejected if p-value < 0.05\")\n",
    "\n",
    "```\n",
    "            \n",
    "            White Test\n",
    "                Ho: Homoscedasticity\n",
    "                Ha: Heteroscedasticity\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "\n",
    "# Fit your regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = model.resid\n",
    "\n",
    "# Get independent variables for White test\n",
    "exog = model.model.exog\n",
    "\n",
    "# Perform White test\n",
    "white_test = het_white(residuals, exog)\n",
    "print(\"White Test Results:\")\n",
    "print(\"LM Statistic:\", white_test[0])\n",
    "print(\"LM-Test p-value:\", white_test[1])\n",
    "print(\"F-Test p-value:\", white_test[2])\n",
    "print(\"Degrees of Freedom:\", white_test[3])\n",
    "```\n",
    "            \n",
    "            NCV test(non-constant variance)\n",
    "                Ho: Homoscedasticity(variance of the errors (residuals) is constant)\n",
    "                Ha: Heteroscedasticity\n",
    "                \n",
    "                \n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.compat import lzip\n",
    "from statsmodels.stats.diagnostic import het_goldfeldquandt\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# Perform the Goldfeld-Quandt test for non-constant variance\n",
    "name = ['F statistic', 'p-value']\n",
    "test = het_goldfeldquandt(results.resid, X)\n",
    "print(lzip(name, test))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c62453c",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f961b7e",
   "metadata": {},
   "source": [
    "    No Autocorrelation : \n",
    "        No correlation on observation of Dependent Variable. \n",
    "        \n",
    "        \n",
    "        Measure :\n",
    "            Durbin Watson Test\n",
    "                Ho is significant we use linear regression\n",
    "                Ha is significant then use Time series\n",
    "                \n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# Perform Durbin-Watson test for autocorrelation in residuals\n",
    "dw_statistic = durbin_watson(results.resid)\n",
    "print(\"Durbin-Watson statistic:\", dw_statistic)\n",
    "```\n",
    "\n",
    "    Interpretation of Durbin-Watson test :\n",
    "        Test statistic ~ 2, => no significant autocorrelation.\n",
    "        Test statistic is significantly < 2 (approaching 0), it suggests positive autocorrelation.\n",
    "        Test statistic is significantly > 2 (approaching 4), it suggests negative autocorrelation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
